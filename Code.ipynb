{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q huggingface_hub llama-cpp-python sentence-transformers gradio torch SpeechRecognition gTTS gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists\n",
      "\n",
      "Model status: Found\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mradermacher/Raya-therapist-model-GGUF\"\n",
    "model_file = \"Raya-therapist-model.Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(model_file):\n",
    "    print(\"Downloading model...\")\n",
    "    hf_hub_download(\n",
    "        repo_id=model_name,\n",
    "        filename=model_file,\n",
    "        local_dir=\".\",\n",
    "        resume_download=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Model already exists\")\n",
    "\n",
    "\n",
    "print(\"\\nModel status:\", \"Found\" if os.path.exists(model_file) else \"Missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    llm = Llama(\n",
    "        model_path=model_file,\n",
    "        n_ctx=16384, \n",
    "        n_gpu_layers=-1,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"LLM initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LLM initialization failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Therapist system prompt\n",
    "system_prompt = (\n",
    "    \"You are a compassionate, empathetic, and professional therapist. \"\n",
    "    \"Your goal is to respond supportively, clearly, and cheerfully. \"\n",
    "    \"Do NOT include any reasoning, explanations, drafts, or thoughts. \"\n",
    "    \"ONLY output the final response for the user, nothing else. \"\n",
    "    \"The response should be concise, friendly, and ready to send as-is.\"\n",
    ")\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "# Convert voice to text\n",
    "def transcribe_audio(audio_path):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            return recognizer.recognize_google(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            return \"(Sorry, I couldn't understand that.)\"\n",
    "        except sr.RequestError:\n",
    "            return \"(Speech recognition service is unavailable.)\"\n",
    "\n",
    "# Main chatbot function\n",
    "def chat_with_voice(user_text, audio_input, chat_history):\n",
    "    if audio_input is not None:\n",
    "        user_message = transcribe_audio(audio_input)\n",
    "    else:\n",
    "        user_message = user_text.strip()\n",
    "\n",
    "    if not user_message:\n",
    "        return \"\", None, chat_history\n",
    "\n",
    "    # Add user message to history\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # Get AI reply\n",
    "    output = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    reply = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "    # Generate TTS audio\n",
    "    import re\n",
    "\n",
    "    # Ensure only the final sentence is kept\n",
    "    # Extract only the final answer (last non-empty line)\n",
    "    reply_lines = [line.strip() for line in reply.split(\"\\n\") if line.strip()]\n",
    "    final_reply = reply_lines[-1] if reply_lines else \"(No response generated.)\"\n",
    "\n",
    "    # Generate TTS audio\n",
    "    tts =tts = gTTS(final_reply, lang=\"en\", tld=\"co.in\")\n",
    "\n",
    "    temp_audio_path = tempfile.mktemp(suffix=\".mp3\")\n",
    "    tts.save(temp_audio_path)\n",
    "\n",
    "    chat_history.append((user_message, final_reply))\n",
    "    return \"\", temp_audio_path, chat_history\n",
    "\n",
    "# Build Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸ§  MIND MATE\")\n",
    "\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    with gr.Row():\n",
    "        text_input = gr.Textbox(label=\"Let's Chat\")\n",
    "        audio_input = gr.Audio(type=\"filepath\", sources=[\"microphone\"], label=\"ðŸŽ¤ Let's Talk\")\n",
    "    output_audio = gr.Audio(label=\"ðŸ”Š Therapist's Voice Reply\", autoplay=True)\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    text_input.submit(chat_with_voice, [text_input, audio_input, chatbot], [text_input, output_audio, chatbot])\n",
    "    audio_input.change(chat_with_voice, [text_input, audio_input, chatbot], [text_input, output_audio, chatbot])\n",
    "    clear.click(lambda: ([], \"\", None), None, [chatbot, text_input, audio_input])\n",
    "    \n",
    "    gr.HTML(\"\"\"\n",
    "<p>Find Help Offline:</p>\n",
    "<ul>\n",
    "<li><h2><a href=\"https://www.amahahealth.com/\" target=\"_blank\">Amaha Health</a></h2></li>\n",
    "<li><h2><a href=\"https://www.centerformentalhealth.in/\" target=\"_blank\">Centre for Mental Health</a></h2></li>\n",
    "</ul>\n",
    "\"\"\")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
